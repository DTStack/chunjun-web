"use strict";(self.webpackChunkchunjun_web=self.webpackChunkchunjun_web||[]).push([[507],{3905:function(e,t,a){a.d(t,{Zo:function(){return m},kt:function(){return k}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function p(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var o=n.createContext({}),u=function(e){var t=n.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},m=function(e){var t=u(e.components);return n.createElement(o.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},s=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,o=e.parentName,m=p(e,["components","mdxType","originalType","parentName"]),s=u(a),k=r,N=s["".concat(o,".").concat(k)]||s[k]||d[k]||l;return a?n.createElement(N,i(i({ref:t},m),{},{components:a})):n.createElement(N,i({ref:t},m))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=s;var p={};for(var o in t)hasOwnProperty.call(t,o)&&(p[o]=t[o]);p.originalType=e,p.mdxType="string"==typeof e?e:r,i[1]=p;for(var u=2;u<l;u++)i[u]=a[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}s.displayName="MDXCreateElement"},9241:function(e,t,a){a.r(t),a.d(t,{assets:function(){return m},contentTitle:function(){return o},default:function(){return k},frontMatter:function(){return p},metadata:function(){return u},toc:function(){return d}});var n=a(7462),r=a(3366),l=(a(7294),a(3905)),i=["components"],p={},o="HDFS Sink",u={unversionedId:"chunjunDocs/connectors/hdfs/hdfs-sink",id:"chunjunDocs/connectors/hdfs/hdfs-sink",title:"HDFS Sink",description:"\u2160\u3001Introduction",source:"@site/docs/chunjunDocs/connectors/hdfs/hdfs-sink.md",sourceDirName:"chunjunDocs/connectors/hdfs",slug:"/chunjunDocs/connectors/hdfs/hdfs-sink",permalink:"/chunjun-web/en/docs/chunjunDocs/connectors/hdfs/hdfs-sink",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chunjunDocs/connectors/hdfs/hdfs-sink.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Hbase Source",permalink:"/chunjun-web/en/docs/chunjunDocs/connectors/hbase/hbase-source"},next:{title:"HDFS Source",permalink:"/chunjun-web/en/docs/chunjunDocs/connectors/hdfs/hdfs-source"}},m={},d=[{value:"\u2160\u3001Introduction",id:"\u2170introduction",level:2},{value:"\u2161\u3001Supported version",id:"\u2171supported-version",level:2},{value:"\u2162\u3001Plugin name",id:"\u2172plugin-name",level:2},{value:"\u2163\u3001Parameter Description",id:"\u2173parameter-description",level:2},{value:"1\u3001Sync",id:"1sync",level:3},{value:"2\u3001SQL",id:"2sql",level:3},{value:"\u2164\u3001Data Type",id:"\u2174data-type",level:2},{value:"\u2165\u3001Script example",id:"\u2175script-example",level:2}],s={toc:d};function k(e){var t=e.components,a=(0,r.Z)(e,i);return(0,l.kt)("wrapper",(0,n.Z)({},s,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"hdfs-sink"},"HDFS Sink"),(0,l.kt)("h2",{id:"\u2170introduction"},"\u2160\u3001Introduction"),(0,l.kt)("p",null,"The HDFS plugin supports reading and writing TextFile, Orc, and Parquet files directly from the configured HDFS path, and is generally used with Hive tables. For example: reading all data in a partition of the Hive table is essentially reading the data files under the HDFS path of the corresponding partition of the Hive table; writing data to a partition of the Hive table is essentially writing the data files directly to the HDFS of the corresponding partition Under the path; the HDFS plugin will not perform any DDL operations on the Hive table.\nHDFS Sink will use two-phase commit when checkpoint is turned on. During pre-commit, the data files generated in the .data directory are copied to the official directory and the copied data files are marked. The data files marked in the .data directory are deleted during the commit phase and rolled back. Delete the data files marked in the official catalog at the time."),(0,l.kt)("h2",{id:"\u2171supported-version"},"\u2161\u3001Supported version"),(0,l.kt)("p",null,"Hadoop 2.x\u3001Hadoop 3.x"),(0,l.kt)("h2",{id:"\u2172plugin-name"},"\u2162\u3001Plugin name"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Sync"),(0,l.kt)("th",{parentName:"tr",align:null},"hdfssink\u3001hdfswriter"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"SQL"),(0,l.kt)("td",{parentName:"tr",align:null},"hdfs-x")))),(0,l.kt)("h2",{id:"\u2173parameter-description"},"\u2163\u3001Parameter Description"),(0,l.kt)("h3",{id:"1sync"},"1\u3001Sync"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"path")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe path of the data file to be written"),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aThe file path actually written is path/fileName"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"fileName")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aData file directory name"),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aThe file path actually written is path/fileName"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"writeMode")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aHDFS Sink data cleaning processing mode before writing\uff1a",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"append"),(0,l.kt)("li",{parentName:"ul"},"overwrite"))),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aAll files in the current directory of hdfs will be deleted in overwrite mode"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1aappend",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"fileType")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1a\u25cb File type, currently only supports user configuration as ",(0,l.kt)("inlineCode",{parentName:"li"},"text"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"orc"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"parquet"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"text\uff1atextfile file format"),(0,l.kt)("li",{parentName:"ul"},"orc\uff1aorcfile file format"),(0,l.kt)("li",{parentName:"ul"},"parquet\uff1aparquet file format"))),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"defaultFS")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aHadoop hdfs file system namenode node address. Format: hdfs://ip:port; for example: hdfs://127.0.0.1:9000"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"column")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aNeed to read the field"),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aDoes not support * format"),(0,l.kt)("li",{parentName:"ul"},"format\uff1a")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-text"},'"column": [{\n    "name": "col",\n    "type": "string",\n    "index":1,\n    "isPart":false,\n    "format": "yyyy-MM-dd hh:mm:ss",\n    "value": "value"\n}]\n')),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"property description:"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"name\uff1arequired\uff0cField Name"),(0,l.kt)("li",{parentName:"ul"},"type\uff1arequired\uff0cField type, which needs to match the actual field type in the data file"),(0,l.kt)("li",{parentName:"ul"},"index\uff1aoptional\uff0cThe position index of the field in all fields, starting from 0, the default is -1, read in sequence in the order of the array, read the specified field column after configuration"),(0,l.kt)("li",{parentName:"ul"},"isPart\uff1aoptional\uff0cWhether it is a partition field, if it is a partition field, the partition assignment will be automatically intercepted from the path, the default is fale"),(0,l.kt)("li",{parentName:"ul"},"format\uff1aoptional\uff0cFormat the date according to the specified format"),(0,l.kt)("li",{parentName:"ul"},"value\uff1aoptional\uff0cConstant field, return the value of value as a constant column"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"required\uff1arequired")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"type\uff1aArray")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"defaults\uff1anone"),(0,l.kt)("br",null)),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"hadoopConfig")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe configuration in core-site.xml and hdfs-site.xml that need to be filled in the cluster HA mode, including kerberos related configuration when kerberos is turned on"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1aMap<String, Object>"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"fieldDelimiter")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description: Field separator when fileType is text"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"\\001"),(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"fullColumnName")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aField name written"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1alist"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1acolumn name collection",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"fullColumnType")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aField type written"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1alist"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1acolumn type collection",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"compress")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1ahdfs file compression type",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"text\uff1aSupport ",(0,l.kt)("inlineCode",{parentName:"li"},"GZIP"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"BZIP2")," format"),(0,l.kt)("li",{parentName:"ul"},"orc\uff1aSupport",(0,l.kt)("inlineCode",{parentName:"li"},"SNAPPY"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"GZIP"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"BZIP"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"LZ4"),"format"),(0,l.kt)("li",{parentName:"ul"},"parquet\uff1aSupport",(0,l.kt)("inlineCode",{parentName:"li"},"SNAPPY"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"GZIP"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"LZO"),"format"))),(0,l.kt)("li",{parentName:"ul"},"notice\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"SNAPPY"),"format requires users to install SnappyCodec"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"- text is not compressed by default\n- orc defaults to ZLIB format\n- parquet defaults to SNAPPY format\n")),(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"maxFileSize")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe maximum size of a single file written to hdfs, in bytes"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1along"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"1073741824"),"\uff081G\uff09",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"nextCheckRows")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe number of intervals for checking the file size next time, and the file size of the current written file will be queried every time this number is reached"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1along"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"5000"),(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"rowGroupSIze")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aSet the size of the row group when the fileType is parquet, in bytes"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1aint"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"134217728"),"\uff08128M\uff09",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"enableDictionary")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aWhen fileType is parquet, whether to start dictionary encoding"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1aboolean"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"true"),(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"encoding")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe character encoding of the field when fileType is text"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"UTF-8"))))),(0,l.kt)("h3",{id:"2sql"},"2\u3001SQL"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"path")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe path of the data file to be written"),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aThe file path actually written is path/fileName"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"file-name")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aData file directory name"),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aThe file path actually written is path/fileName"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"write-mode")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aHDFS Sink data cleaning processing mode before writing\uff1a",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"append"),(0,l.kt)("li",{parentName:"ul"},"overwrite"))),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aAll files in the current directory of hdfs will be deleted in overwrite mode"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1aappend",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"file-type")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1a\u25cb File type, currently only supports user configuration as ",(0,l.kt)("inlineCode",{parentName:"li"},"text"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"orc"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"parquet"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"text\uff1atextfile file format"),(0,l.kt)("li",{parentName:"ul"},"orc\uff1aorcfile file format"),(0,l.kt)("li",{parentName:"ul"},"parquet\uff1aparquet file format"))),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"default-fs")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aHadoop hdfs file system namenode node address. Format: hdfs://ip:port; for example: hdfs://127.0.0.1:9000"),(0,l.kt)("li",{parentName:"ul"},"required\uff1arequired"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"column")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aNeed to read the field"),(0,l.kt)("li",{parentName:"ul"},"notice\uff1aDoes not support * format"),(0,l.kt)("li",{parentName:"ul"},"format\uff1a")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-text"},'"column": [{\n    "name": "col",\n    "type": "string",\n    "index":1,\n    "isPart":false,\n    "format": "yyyy-MM-dd hh:mm:ss",\n    "value": "value"\n}]\n')),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"property description:"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"name\uff1arequired\uff0cField Name"),(0,l.kt)("li",{parentName:"ul"},"type\uff1arequired\uff0cField type, which needs to match the actual field type in the data file"),(0,l.kt)("li",{parentName:"ul"},"index\uff1aoptional\uff0cThe position index of the field in all fields, starting from 0, the default is -1, read in sequence in the order of the array, read the specified field column after configuration"),(0,l.kt)("li",{parentName:"ul"},"isPart\uff1aoptional\uff0cWhether it is a partition field, if it is a partition field, the partition assignment will be automatically intercepted from the path, the default is fale"),(0,l.kt)("li",{parentName:"ul"},"format\uff1aoptional\uff0cFormat the date according to the specified format"),(0,l.kt)("li",{parentName:"ul"},"value\uff1aoptional\uff0cConstant field, return the value of value as a constant column"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"required\uff1arequired")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"type\uff1aArray")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"defaults\uff1anone"),(0,l.kt)("br",null)),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"hadoopConfig")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe configuration in core-site.xml and hdfs-site.xml that need to be filled in the cluster HA mode, including kerberos related configuration when kerberos is turned on"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone"),(0,l.kt)("li",{parentName:"ul"},"configuration method\uff1a'properties.key' ='value', key is the key in hadoopConfig, and value is the value in hadoopConfig, as shown below:")))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"'properties.hadoop.user.name' = 'root',\n'properties.dfs.ha.namenodes.ns' = 'nn1,nn2',\n'properties.fs.defaultFS' = 'hdfs://ns',\n'properties.dfs.namenode.rpc-address.ns.nn2' = 'ip:9000',\n'properties.dfs.client.failover.proxy.provider.ns' = 'org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider',\n'properties.dfs.namenode.rpc-address.ns.nn1' = 'ip:9000',\n'properties.dfs.nameservices' = 'ns',\n'properties.fs.hdfs.impl.disable.cache' = 'true',\n'properties.fs.hdfs.impl' = 'org.apache.hadoop.hdfs.DistributedFileSystem'\n")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"field-delimiter")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description: Field separator when fileType is text"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"\\001"),(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"compress")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1ahdfs file compression type",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"text\uff1aSupport ",(0,l.kt)("inlineCode",{parentName:"li"},"GZIP"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"BZIP2")," format"),(0,l.kt)("li",{parentName:"ul"},"orc\uff1aSupport",(0,l.kt)("inlineCode",{parentName:"li"},"SNAPPY"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"GZIP"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"BZIP"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"LZ4"),"format"),(0,l.kt)("li",{parentName:"ul"},"parquet\uff1aSupport",(0,l.kt)("inlineCode",{parentName:"li"},"SNAPPY"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"GZIP"),"\u3001",(0,l.kt)("inlineCode",{parentName:"li"},"LZO"),"format"))),(0,l.kt)("li",{parentName:"ul"},"notice\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"SNAPPY"),"format requires users to install SnappyCodec"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"text is not compressed by default"),(0,l.kt)("li",{parentName:"ul"},"orc defaults to ZLIB format"),(0,l.kt)("li",{parentName:"ul"},"parquet defaults to SNAPPY format",(0,l.kt)("br",null)))))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"max-file-size")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe maximum size of a single file written to hdfs, in bytes"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1along"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"1073741824"),"\uff081G\uff09",(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"next-check-rows")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe number of intervals for checking the file size next time, and the file size of the current written file will be queried every time this number is reached"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1along"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"5000"),(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"enable-dictionary")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aWhen fileType is parquet, whether to start dictionary encoding"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1aboolean"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"true"),(0,l.kt)("br",null)))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"encoding")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1aThe character encoding of the field when fileType is text"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1a",(0,l.kt)("inlineCode",{parentName:"li"},"UTF-8")))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"sink.parallelism")),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"description\uff1asink parallelism"),(0,l.kt)("li",{parentName:"ul"},"required\uff1aoptional"),(0,l.kt)("li",{parentName:"ul"},"type\uff1astring"),(0,l.kt)("li",{parentName:"ul"},"defaults\uff1anone",(0,l.kt)("br",null))))),(0,l.kt)("h2",{id:"\u2174data-type"},"\u2164\u3001Data Type"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Support"),(0,l.kt)("th",{parentName:"tr",align:null},"BOOLEAN\u3001TINYINT\u3001SMALLINT\u3001INT\u3001BIGINT\u3001FLOAT\u3001DOUBLE\u3001DECIMAL\u3001STRING\u3001VARCHAR\u3001CHAR\u3001TIMESTAMP\u3001DATE\u3001BINARY"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Not supported yet"),(0,l.kt)("td",{parentName:"tr",align:null},"ARRAY\u3001MAP\u3001STRUCT\u3001UNION")))),(0,l.kt)("h2",{id:"\u2175script-example"},"\u2165\u3001Script example"),(0,l.kt)("p",null,"See the ",(0,l.kt)("inlineCode",{parentName:"p"},"flinkx-examples")," folder in the project."))}k.isMDXComponent=!0}}]);